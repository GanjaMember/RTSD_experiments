{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3565970,"sourceType":"datasetVersion","datasetId":2050294}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Исследование и оценка методов и архитектур на задаче детекции дорожных знаков","metadata":{}},{"cell_type":"markdown","source":"Код был взят из семинара по детекции курса DLS. Это модифицированная под RTSD версия","metadata":{}},{"cell_type":"code","source":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport gc\nimport os\nimport math\nimport json\nfrom functools import partial\nfrom collections import Counter, defaultdict\n\nimport io\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom torchvision import transforms, models\nfrom torchvision.ops import nms, box_iou\nfrom torchvision.models.detection.anchor_utils import AnchorGenerator\nfrom torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n\nfrom torchmetrics.detection import MeanAveragePrecision\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:19:53.618898Z","iopub.execute_input":"2025-07-13T18:19:53.619278Z","iopub.status.idle":"2025-07-13T18:19:53.629020Z","shell.execute_reply.started":"2025-07-13T18:19:53.619253Z","shell.execute_reply":"2025-07-13T18:19:53.628005Z"}},"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Подготовка данных","metadata":{"id":"42e2b5b2-113e-43d5-a134-31b51a256671"}},{"cell_type":"code","source":"os.listdir(\"/kaggle/input/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:19:53.630777Z","iopub.execute_input":"2025-07-13T18:19:53.631141Z","iopub.status.idle":"2025-07-13T18:19:53.673310Z","shell.execute_reply.started":"2025-07-13T18:19:53.631117Z","shell.execute_reply":"2025-07-13T18:19:53.672193Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"['rtsd-small']"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/rtsd-small\"\nanno_path = dataset_path + \"/full_gt_reduced.csv\"\nimage_dir = dataset_path + \"/rtsd-dataset-reduced/rtsd-dataset-reduced/rtsd-frames\"\n# label_map = dataset_path + \"/rtsd-dataset-reduced/rtsd-dataset-reduced/label_map.json\"\n\n# Временно отфильтруем классы\nfour_classes = (\"6_16\", )  # , \"5_19_1\", \"2_1\", \"2_4\"\n\n# with open(label_map, 'r') as f:\n    # label_map = json.load(f)\nlabel_map = {cls: i for i, cls in enumerate(four_classes)}  # DEBUG\nlabel_map = {k: v for k, v in label_map.items()}  # не забывай, что классы начинаются с 0!!!\nlabel_map_reverse = {v: k for k, v in label_map.items()}\n\nclass_to_color = [tuple(np.random.choice(range(256), size=3)) for _ in range(len(label_map))]\n\ndf = pd.read_csv(anno_path)\n\n# Фильтруем классы\ndf = df[df[\"sign_class\"].isin(label_map.keys())]\n\n# Переводим колонки координат в списки\ndf['bboxes'] = df[['x_from', 'y_from', 'width', 'height']].apply(lambda row: [row['x_from'], row['y_from'], row['width'], row['height']], axis=1)\n\n# Переводим названия знаков в их индексы\ndf['sign_class'] = df[['sign_class']].apply(lambda row: label_map[row['sign_class']], axis=1)\n\n# Удаляем ненужные колонки\ndf = df.drop(\n    columns=[\"Unnamed: 0\", \"is_train\", 'x_from', 'y_from', 'width', 'height', \"sign_id\"],\n    errors=\"ignore\"\n)\n\ndf = df.rename(columns={\"sign_class\": \"labels\"})\n\n# Группируем разметку разных знаков в одну запись\ndf = df.groupby('filename').agg({\n    'labels': lambda x: list(x),\n    'bboxes': lambda x: list(x)\n}).reset_index()","metadata":{"id":"b2eecb64-16a2-4b97-b627-890ea316a594","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:19:53.674842Z","iopub.execute_input":"2025-07-13T18:19:53.675118Z","iopub.status.idle":"2025-07-13T18:19:53.761182Z","shell.execute_reply.started":"2025-07-13T18:19:53.675090Z","shell.execute_reply":"2025-07-13T18:19:53.760230Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from skmultilearn.model_selection import IterativeStratification\n\ndef stratified_train_test_split(df, test_size=0.3, random_state=None):\n    # Create multi-hot encoded labels\n    all_sign_ids = sorted(set(sign_id for sublist in df['labels'] for sign_id in sublist))\n    y = np.zeros((len(df), len(all_sign_ids)), dtype=int)\n    \n    for i, sign_ids in enumerate(df['labels']):\n        for sign_id in sign_ids:\n            col_idx = all_sign_ids.index(sign_id)\n            y[i, col_idx] = 1\n\n    stratifier = IterativeStratification(\n        n_splits=2,\n        order=1,\n        sample_distribution_per_fold=[1-test_size, test_size],\n        random_state=random_state\n    )\n    test_indices, train_indices = next(stratifier.split(df.index.values, y))\n\n    return df.iloc[train_indices], df.iloc[test_indices]\n\ndef verify_stratification(df_train, df_test):\n    # Посчитать появление каждого класса\n    all_sign_ids = sorted(set(sign_id for sublist in pd.concat([df_train, df_test])['labels'] \n                          for sign_id in sublist))\n    \n    results = []\n    for sign_id in all_sign_ids:\n        train_count = sum(sign_id in signs for signs in df_train['labels'])\n        test_count = sum(sign_id in signs for signs in df_test['labels'])\n        total = train_count + test_count\n        \n        if total > 0:\n            train_pct = (train_count / total) * 100\n            test_pct = (test_count / total) * 100\n            results.append((sign_id, f\"{train_pct:.1f}% / {test_pct:.1f}%\", train_count, test_count))\n    \n    # Create comparison report\n    report = pd.DataFrame(results, \n                         columns=['labels', 'train% / test%', 'train_count', 'test_count'])\n    \n    print(\"Stratification Verification Report:\")\n    print(report.to_string(index=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:19:53.762461Z","iopub.execute_input":"2025-07-13T18:19:53.762955Z","iopub.status.idle":"2025-07-13T18:19:53.809026Z","shell.execute_reply.started":"2025-07-13T18:19:53.762793Z","shell.execute_reply":"2025-07-13T18:19:53.808054Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"df_train, df_valid = stratified_train_test_split(df, test_size=0.2, random_state=None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:19:53.810744Z","iopub.execute_input":"2025-07-13T18:19:53.811083Z","iopub.status.idle":"2025-07-13T18:19:53.825183Z","shell.execute_reply.started":"2025-07-13T18:19:53.811060Z","shell.execute_reply":"2025-07-13T18:19:53.824051Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"print(df_train.shape)\ndf_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:19:53.826266Z","iopub.execute_input":"2025-07-13T18:19:53.826640Z","iopub.status.idle":"2025-07-13T18:19:53.870922Z","shell.execute_reply.started":"2025-07-13T18:19:53.826589Z","shell.execute_reply":"2025-07-13T18:19:53.870092Z"}},"outputs":[{"name":"stdout","text":"(42, 3)\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                            filename labels                  bboxes\n1  autosave02_10_2012_11_55_25_2.jpg    [0]    [[194, 260, 49, 21]]\n3  autosave02_10_2012_12_06_34_0.jpg    [0]    [[718, 264, 54, 24]]\n5  autosave02_10_2012_12_07_52_1.jpg    [0]  [[1174, 191, 105, 46]]\n7  autosave02_10_2012_12_55_36_3.jpg    [0]   [[844, 172, 103, 36]]\n9  autosave09_10_2012_10_10_03_3.jpg    [0]    [[881, 332, 40, 19]]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>labels</th>\n      <th>bboxes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>autosave02_10_2012_11_55_25_2.jpg</td>\n      <td>[0]</td>\n      <td>[[194, 260, 49, 21]]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>autosave02_10_2012_12_06_34_0.jpg</td>\n      <td>[0]</td>\n      <td>[[718, 264, 54, 24]]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>autosave02_10_2012_12_07_52_1.jpg</td>\n      <td>[0]</td>\n      <td>[[1174, 191, 105, 46]]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>autosave02_10_2012_12_55_36_3.jpg</td>\n      <td>[0]</td>\n      <td>[[844, 172, 103, 36]]</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>autosave09_10_2012_10_10_03_3.jpg</td>\n      <td>[0]</td>\n      <td>[[881, 332, 40, 19]]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"Создаем датасет для предобработки данных","metadata":{"id":"1ade755e-4471-4c79-84bc-8b560484e833"}},{"cell_type":"code","source":"class RTSDDataset(Dataset):\n    def __init__(self, dataframe, image_dir, transform=None):\n        self.image_dir = image_dir\n        self.data = dataframe\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        \"\"\"Загружаем данные и разметку для объекта с индексом `idx`.\n\n        labels: List[int] Набор классов для каждого ббокса,\n        boxes: List[List[int]] Набор ббоксов в формате (x_min, y_min, w, h).\n        \"\"\"\n        row = self.data.iloc[idx]\n        image_name = row['filename']\n        image_path = os.path.join(self.image_dir, image_name)\n        image = Image.open(image_path).convert(\"RGB\")\n        image = np.array(image)  # uint8 (0-255) HWC формат\n\n        target = {}\n        target[\"image_id\"] = idx\n\n        if self.transform is not None:\n            transformed = self.transform(image=image, bboxes=row[\"bboxes\"], labels=row[\"labels\"])\n            image, boxes, labels = transformed[\"image\"], transformed[\"bboxes\"], transformed[\"labels\"]\n        else:\n            image = transforms.ToTensor()(image)\n\n        # Get dimensions (works for both tensor and numpy)\n        if isinstance(image, torch.Tensor):\n            _, h, w = image.shape\n        else:\n            h, w = image.shape[:2]\n\n        # Нормализуем координаты ббоксов к [0, 1]\n        # normalized_bboxes = []\n\n        # for bbox in boxes:\n        #     x_min, y_min, width, height = bbox\n\n        #     # Normalize coordinates\n        #     x_min_norm = x_min / w\n        #     y_min_norm = y_min / h\n        #     width_norm = width / w\n        #     height_norm = height / h\n\n        #     normalized_bboxes.append([x_min_norm, y_min_norm, width_norm, height_norm])\n\n        # normalized_bboxes = np.array(normalized_bboxes, dtype=np.float32)\n        \n        # boxes = normalized_bboxes\n        target['boxes'] = torch.tensor(np.array(boxes), dtype=torch.float32)\n        target['labels'] = torch.tensor(labels, dtype=torch.int64)\n\n        return image, target\n\ndef collate_fn(batch):\n    batch = tuple(zip(*batch))\n    images = torch.stack(batch[0])\n    return images, batch[1]","metadata":{"id":"5a2f8ba9-5ae2-4d33-9d9d-5a0f0bb36d38","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:19:53.872089Z","iopub.execute_input":"2025-07-13T18:19:53.872450Z","iopub.status.idle":"2025-07-13T18:19:53.882724Z","shell.execute_reply.started":"2025-07-13T18:19:53.872419Z","shell.execute_reply":"2025-07-13T18:19:53.881559Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def compute_dataset_statistics(dataset, batch_size=16):\n    \"\"\"Compute mean and std of dataset in 0-255 range\"\"\"\n    loader = DataLoader(\n        dataset, \n        batch_size=batch_size, \n        shuffle=False,\n        collate_fn=lambda x: tuple(zip(*x))  # Simple collate for images\n    )\n    \n    channel_sum = np.zeros(3)\n    channel_sq_sum = np.zeros(3)\n    total_pixels = 0\n    \n    for batch in tqdm(loader, desc=\"Computing statistics\"):\n        images = batch[0]  # List of numpy images (H, W, 3)\n\n        images = list(images)\n        for i in range(len(images)):\n            # Convert to tensor if not already\n            if not isinstance(images[i], torch.Tensor):\n                # Convert numpy (H,W,C) to tensor (C,H,W)\n                images[i] = torch.from_numpy(images[i]).permute(2, 0, 1).float()\n\n        # Stack images and convert to float64 for precision\n        batch_array = np.stack(images).astype(np.float32)\n        batch_array = np.transpose(batch_array, (0, 2, 3, 1))\n        b, h, w, c = batch_array.shape\n        \n        # Sum across batch and spatial dimensions\n        channel_sum += batch_array.sum(axis=(0, 1, 2))\n        channel_sq_sum += (batch_array ** 2).sum(axis=(0, 1, 2))\n        total_pixels += b * h * w\n\n    # Calculate final statistics\n    mean = channel_sum / total_pixels\n    std = np.sqrt(channel_sq_sum / total_pixels - mean ** 2)\n    \n    return mean.tolist(), std.tolist()","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-07-13T18:19:53.883931Z","iopub.execute_input":"2025-07-13T18:19:53.884241Z","iopub.status.idle":"2025-07-13T18:19:53.910462Z","shell.execute_reply.started":"2025-07-13T18:19:53.884221Z","shell.execute_reply":"2025-07-13T18:19:53.909361Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# stats_transform = A.Compose([\n#     A.Resize(height=img_height, width=img_width),\n#     # ToTensorV2()\n# ], bbox_params=A.BboxParams(format='coco', label_fields=['labels']))\n\n# # Создать датасет для расчета статистики\n# stats_dataset = RTSDDataset(\n#     df_train,  # Use full training data\n#     image_dir,\n#     transform=stats_transform\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:19:53.911468Z","iopub.execute_input":"2025-07-13T18:19:53.911790Z","iopub.status.idle":"2025-07-13T18:19:53.932012Z","shell.execute_reply.started":"2025-07-13T18:19:53.911763Z","shell.execute_reply":"2025-07-13T18:19:53.930588Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# mean, std = compute_dataset_statistics(stats_dataset)\n# mean = np.array(mean) / 255\n# std = np.array(std) / 255\n\n# print(f\"Mean: {mean}\")\n# print(f\"Std:  {std}\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-07-13T18:19:53.933180Z","iopub.execute_input":"2025-07-13T18:19:53.933514Z","iopub.status.idle":"2025-07-13T18:19:53.959810Z","shell.execute_reply.started":"2025-07-13T18:19:53.933481Z","shell.execute_reply":"2025-07-13T18:19:53.958677Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"img_width = 640\nimg_height = 640\n\nmean = (0.485, 0.456, 0.406)  # Статистика по датасету ImageNet, не отличается от моего\nstd = (0.229, 0.224, 0.225)\n\ntrain_transform = A.Compose([\n    A.Resize(height=img_height, width=img_width),\n    A.Normalize(mean=mean, std=std),\n    ToTensorV2(),\n], bbox_params=A.BboxParams(format='coco', label_fields=['labels']))\n\ntest_transform = A.Compose([\n    A.Resize(height=img_height, width=img_width),\n    A.Normalize(mean=mean, std=std),\n    ToTensorV2(),\n], bbox_params=A.BboxParams(format='coco', label_fields=['labels']))","metadata":{"id":"9c8c7095-9b77-4716-86cd-9b3e7dc3890c","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:19:53.963548Z","iopub.execute_input":"2025-07-13T18:19:53.963906Z","iopub.status.idle":"2025-07-13T18:19:53.987147Z","shell.execute_reply.started":"2025-07-13T18:19:53.963885Z","shell.execute_reply":"2025-07-13T18:19:53.985964Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"batch_size = 2\n\ntrain_dataset = RTSDDataset(df_train, image_dir, transform=train_transform)\nvalid_dataset = RTSDDataset(df_valid, image_dir, transform=test_transform)\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)","metadata":{"id":"e4a10a88-6df0-4d1a-8640-eaca2f12e511","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:19:53.988378Z","iopub.execute_input":"2025-07-13T18:19:53.988761Z","iopub.status.idle":"2025-07-13T18:19:54.008343Z","shell.execute_reply.started":"2025-07-13T18:19:53.988731Z","shell.execute_reply":"2025-07-13T18:19:54.007321Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"Посмотрим на некоторые статистики по датасету","metadata":{}},{"cell_type":"code","source":"def train_info(train_dataloader):\n    \"\"\" Считаем статистики по датасету и рисуем распределение размеров ббоксов по классам. \"\"\"\n    image_min_box_count, image_max_box_count, image_mean_box_count = float('inf'), 0, 0\n    total_box_count = 0\n    total_image_count = 0\n    objects_per_class = {k: 0 for k, v in label_map_reverse.items()}\n    bboxes_sizes = defaultdict(list)\n    labels = set()\n    for images, targets in tqdm(train_dataloader, desc=\"Train Dataset Info\"):\n        for i in range(images.shape[0]):\n            bboxes = targets[i][\"boxes\"]\n            categories = targets[i][\"labels\"]\n\n            # Переводим элементы списка из тензоров в скаляры\n            categories = [category.item() for category in categories]\n\n            total_box_count += len(bboxes)\n            total_image_count += len(images)\n\n            image_min_box_count = min(len(bboxes), image_min_box_count)\n            image_max_box_count = max(len(bboxes), image_min_box_count)\n        \n            for bb, cls in zip(bboxes, categories):\n                labels.add(cls)\n                \n                objects_per_class[cls] += 1\n\n                bboxes_sizes[cls].append(list(bb[2:]))\n\n    bboxes_sizes = dict(sorted(bboxes_sizes.items()))\n\n    print(f\"Min bboxes per image: {image_min_box_count}\")\n    print(f\"Max bboxes per image: {image_max_box_count}\")\n    print(f\"Mean bboxes per image: {total_box_count / total_image_count}\")\n    print(\"\\n\\n\")\n\n    msg = [f\"{label_map_reverse[k]} : {v}\" for k, v in objects_per_class.items()]\n    print(\"Number of object per class:\\n\" + \"\\n\".join(msg))\n    print(\"\\n\")\n\n    print(\"\\nMean bbox size per class:\")\n    for cls, boxes_list in bboxes_sizes.items():\n        print(f\"{label_map_reverse[cls]} : {np.mean(boxes_list, axis=0)}\")\n\n    _, axes = plt.subplots(1, 2, figsize=(12, 4))\n    x_boxes = [np.array(val)[:, 0] for val in bboxes_sizes.values()]\n    y_boxes = [np.array(val)[:, 1] for val in bboxes_sizes.values()]\n\n    for ax, box, direction in zip(axes, [x_boxes, y_boxes], [\"width\", \"height\"]):\n        bplot = ax.boxplot(box, patch_artist=True, labels=labels)\n        for patch, color in zip(bplot[\"boxes\"], class_to_color):\n            patch.set_facecolor(np.array(color) / 255)\n        ax.set_ylabel(f\"Bbox size by {direction}\")\n        ax.set_title(f\"Bboxes distribution per class by {direction}\")\n\n# box_sizes = train_info(train_dataloader)","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-07-13T18:19:54.009487Z","iopub.execute_input":"2025-07-13T18:19:54.009861Z","iopub.status.idle":"2025-07-13T18:19:54.036879Z","shell.execute_reply.started":"2025-07-13T18:19:54.009831Z","shell.execute_reply":"2025-07-13T18:19:54.035804Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Возьмем один батч\nbatch_index, i = 1, 0\nfor images, targets in train_dataloader:\n    i += 1\n    if i == batch_index:\n        break","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-07-13T18:19:54.037932Z","iopub.execute_input":"2025-07-13T18:19:54.038263Z","iopub.status.idle":"2025-07-13T18:19:54.273069Z","shell.execute_reply.started":"2025-07-13T18:19:54.038238Z","shell.execute_reply":"2025-07-13T18:19:54.271998Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def denormalize(image_tensor, mean, std):\n    \"\"\"Обратная нормализация для отображения изображения\"\"\"\n    mean = torch.tensor(mean).view(3, 1, 1)\n    std = torch.tensor(std).view(3, 1, 1)\n    return image_tensor * std + mean\n\n# Вспомогательные функции для отрисовки данных\ndef add_bbox(image, box, label='', color=(128, 128, 128), txt_color=(0, 0, 0), lw=2, normalized=False, title=\"\"):\n    x_min, y_min, width, height = box\n    \n    if normalized:\n        h, w = image.shape[:2]\n        x1 = int(x_min * w)\n        y1 = int(y_min * h)\n        x2 = int((x_min + width) * w)\n        y2 = int((y_min + height) * h)\n    else:\n        x1 = int(x_min)\n        y1 = int(y_min)\n        x2 = int(x_min + width)\n        y2 = int(y_min + height)\n\n    lw = int(max(round(sum(image.shape) / 2 * 0.003), lw))\n    \n    h, w = image.shape[:2]\n\n    color = tuple(map(int, color))\n    \n    cv2.rectangle(image, (x1, y1), (x2, y2), color, thickness=lw, lineType=cv2.LINE_AA)\n\n    if label:\n        tf = max(lw - 1, 1)\n        w, h = cv2.getTextSize(label, 0, fontScale=lw / 3, thickness=tf)[0]\n        outside = y1 - h >= 3\n        x2, y2 = x1 + w, y1 - h - 3 if outside else y1 + h + 3\n\n        cv2.rectangle(image, (x1, y1), (x2, y2), color, -1, cv2.LINE_AA)\n        cv2.putText(\n            image,\n            label,\n            (x1, y1 - 2 if outside else y1 + h + 2),\n            0,\n            lw / 3,\n            txt_color,\n            thickness=tf,\n            lineType=cv2.LINE_AA\n        )\n\n    return image\n\ndef plot_examples(images, targets=None, predictions=None, image_norm=None, indices=None, num_examples=6, row_figsize=(12, 3), title=\"\"):\n    if indices is None:\n        indices = np.random.choice(len(df), size=num_examples, replace=False)\n    else:\n        num_examples = len(indices)\n\n    ncols = min(num_examples, 3)\n    nrows = math.ceil(num_examples / 3)\n    \n    fig, axes = plt.subplots(nrows, ncols, figsize=(row_figsize[0], row_figsize[1] * nrows), tight_layout=True)\n    axes = axes.reshape(-1)\n\n    if title:\n        fig.suptitle(title, fontsize=16, y=1.05)\n\n    for i, ax in zip(range(min(len(images), num_examples)), axes):\n        image = images[i].clone().detach().cpu()\n        \n        # 1. Денормализуем изображение\n        if image_norm:\n            mean, std = image_norm\n            image = denormalize(image, mean, std)\n        \n        # 2. Переводим в формат для OpenCV\n        image = image.mul(255).clamp(0, 255).permute(1, 2, 0).numpy()\n        image = image.astype(np.uint8)\n        image_bgr = np.ascontiguousarray(image[..., ::-1])  # RGB -> BGR\n\n        if targets:\n            bboxes = targets[i][\"boxes\"]\n            classes = targets[i][\"labels\"]\n            for bbox, label in zip(bboxes, classes):\n                color = class_to_color[label]\n                class_name = label_map_reverse[label.item()]\n                img = add_bbox(image_bgr, bbox, label=str(class_name), color=(255, 0, 0), lw=2)\n\n        if predictions:\n            preds = predictions[i]\n            for bbox, label, score in zip(preds[\"boxes\"], preds[\"labels\"], preds[\"scores\"]):\n                color = class_to_color[label]\n                if isinstance(label, torch.Tensor):\n                    label = label.item()\n                label = label_map_reverse[label]\n                img = add_bbox(img, bbox, label=f\"Class {label}: {score:.2f}\", color=(0, 0, 255))\n\n        # Переводим в rgb обратно для matplotlib\n        image_rgb = image_bgr[..., ::-1]\n        ax.imshow(image_rgb)\n        ax.set_title(f\"Image id: {i}\")\n        ax.set_xticks([])\n        ax.set_yticks([])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:19:54.274321Z","iopub.execute_input":"2025-07-13T18:19:54.274754Z","iopub.status.idle":"2025-07-13T18:19:54.295337Z","shell.execute_reply.started":"2025-07-13T18:19:54.274725Z","shell.execute_reply":"2025-07-13T18:19:54.294320Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# plot_examples(images, targets=targets, image_norm=(mean, std), num_examples=min(len(images), 3), title=\"Image samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:19:54.296448Z","iopub.execute_input":"2025-07-13T18:19:54.296752Z","iopub.status.idle":"2025-07-13T18:19:54.323359Z","shell.execute_reply.started":"2025-07-13T18:19:54.296730Z","shell.execute_reply":"2025-07-13T18:19:54.322330Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## Архитектура модели","metadata":{"id":"adb463da-ab51-460c-bfb9-e95dddfff90f"}},{"cell_type":"markdown","source":"### Backbone","metadata":{}},{"cell_type":"code","source":"pretrained_model = models.efficientnet_b7(weights='DEFAULT')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:19:54.324447Z","iopub.execute_input":"2025-07-13T18:19:54.325119Z","iopub.status.idle":"2025-07-13T18:19:58.058544Z","shell.execute_reply.started":"2025-07-13T18:19:54.325090Z","shell.execute_reply":"2025-07-13T18:19:58.057716Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/efficientnet_b7_lukemelas-c5b4e57e.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b7_lukemelas-c5b4e57e.pth\n100%|██████████| 255M/255M [00:01<00:00, 173MB/s]  \n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"return_nodes = {\n    '0': '0',\n    '1.0.block.2': '1',\n    '2.1.add': '2',\n    '3.1.add': '3',\n    '4.2.add': '4',\n    '5.2.add': '5',\n    '6.3.add': '6',\n    '7.0.block.3': '7',\n    '8': '8',\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:19:58.059852Z","iopub.execute_input":"2025-07-13T18:19:58.060167Z","iopub.status.idle":"2025-07-13T18:19:58.064527Z","shell.execute_reply.started":"2025-07-13T18:19:58.060147Z","shell.execute_reply":"2025-07-13T18:19:58.063418Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"class Backbone(nn.Module):\n    def __init__(self, pretrained_model, return_nodes, unfreeze_block=None):\n        super().__init__()\n        self.backbone = pretrained_model.features\n\n        for param in self.backbone.parameters():\n            param.requires_grad = False\n\n        if unfreeze_block is not None:\n            for param in self.backbone[-unfreeze_block:].parameters():\n                param.requires_grad = True\n\n        self.backbone = create_feature_extractor(\n            self.backbone, return_nodes=return_nodes)\n\n    def forward(self, x):\n        return self.backbone(x)","metadata":{"id":"d92b9c64-73f8-4195-aa56-b9108589a312","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:19:58.065569Z","iopub.execute_input":"2025-07-13T18:19:58.065897Z","iopub.status.idle":"2025-07-13T18:19:58.093560Z","shell.execute_reply.started":"2025-07-13T18:19:58.065870Z","shell.execute_reply":"2025-07-13T18:19:58.092697Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"### Neck","metadata":{"id":"3b3a3748-aa11-4bf8-9b38-59ff08366573"}},{"cell_type":"code","source":"class ConvTransposeBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.ConvTranspose2d(\n            in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.silu = nn.SiLU()\n\n    def forward(self, x):\n        return self.silu(self.bn(self.conv(x)))","metadata":{"id":"2119ccb8-7d21-4805-af5c-5a1d8d3a0e06","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:19:58.094677Z","iopub.execute_input":"2025-07-13T18:19:58.094994Z","iopub.status.idle":"2025-07-13T18:19:58.117528Z","shell.execute_reply.started":"2025-07-13T18:19:58.094966Z","shell.execute_reply":"2025-07-13T18:19:58.116199Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels,\n                              kernel_size=3, stride=1, padding=1)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.silu = nn.SiLU()\n\n    def forward(self, x):\n        return self.silu(self.bn(self.conv(x)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:19:58.118839Z","iopub.execute_input":"2025-07-13T18:19:58.119387Z","iopub.status.idle":"2025-07-13T18:19:58.140764Z","shell.execute_reply.started":"2025-07-13T18:19:58.119361Z","shell.execute_reply":"2025-07-13T18:19:58.139709Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"class Neck(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.conv0 = ConvBlock(2560, 320)\n        self.conv1 = ConvBlock(960, 192)\n        self.conv2 = ConvBlock(576, 112)\n        self.conv_transpose1 = ConvTransposeBlock(112, 112)\n        self.conv3 = ConvBlock(336, 80)\n        self.conv4 = ConvBlock(240, 40)\n\n    def forward(self, x, encoder_outputs):\n        x = self.conv0(x)\n        x = torch.cat([x, encoder_outputs[7]], dim=1)\n        x = self.conv1(x)\n        x = torch.cat([x, encoder_outputs[6]], dim=1)\n        x = self.conv2(x)\n        x = self.conv_transpose1(x)\n        x = torch.cat([x, encoder_outputs[5]], dim=1)\n        x = self.conv3(x)\n        x = torch.cat([x, encoder_outputs[4]], dim=1)\n        x = self.conv4(x)\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:19:58.141833Z","iopub.execute_input":"2025-07-13T18:19:58.142116Z","iopub.status.idle":"2025-07-13T18:19:58.166462Z","shell.execute_reply.started":"2025-07-13T18:19:58.142086Z","shell.execute_reply":"2025-07-13T18:19:58.165127Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"### Head","metadata":{"id":"531f59fe-8248-4b19-aa31-8acecccb0828"}},{"cell_type":"code","source":"class DetectionHead(nn.Module):\n    def __init__(self, in_channels, num_anchors, num_classes):\n        super().__init__()\n        self.neck = nn.Conv2d(in_channels, in_channels,\n                              kernel_size=1, padding=0)\n        self.cls_head = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),\n            nn.SiLU(),\n            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),\n            nn.SiLU(),\n            nn.Conv2d(in_channels, num_anchors *\n                      num_classes, kernel_size=1, padding=0),\n        )\n        self.bbox_neck = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),\n            nn.SiLU(),\n            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),\n            nn.SiLU(),\n        )\n        self.reg_head = nn.Conv2d(\n            in_channels, num_anchors * 4, kernel_size=1, padding=0)\n        self.obj_head = nn.Conv2d(\n            in_channels, num_anchors * 1, kernel_size=1, padding=0)\n\n        self.num_classes = num_classes\n\n    def forward(self, x):\n        x = F.silu(self.neck(x))\n\n        cls_logits = self.cls_head(x)\n\n        bbox_neck = self.bbox_neck(x)\n\n        bbox_reg = self.reg_head(bbox_neck)\n        confidence_score = self.obj_head(bbox_neck)\n\n        return bbox_reg, confidence_score, cls_logits","metadata":{"id":"f223b6ca-9498-4e75-9b97-6199c82977e1","trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-07-13T18:19:58.167417Z","iopub.execute_input":"2025-07-13T18:19:58.167685Z","iopub.status.idle":"2025-07-13T18:19:58.193897Z","shell.execute_reply.started":"2025-07-13T18:19:58.167665Z","shell.execute_reply":"2025-07-13T18:19:58.193051Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"### Detector","metadata":{}},{"cell_type":"code","source":"class Detector(nn.Module):\n    def __init__(self, pretrained_model,\n                 return_nodes,\n                 unfreeze_block,\n                 input_size=(640, 640),\n                 anchor_sizes=(32, 64, 128),\n                 anchor_ratios=(0.5, 1.0, 2.0),\n                 num_classes=4):\n        super().__init__()\n\n        self.stride = 16 # во сколько раз уменьшаеться фиче мапа после backbone\n        feature_map_size = (input_size[0] // self.stride, input_size[1] // self.stride)\n        print(\"feature_map_size\", feature_map_size)\n        self.backbone = Backbone(\n            pretrained_model, return_nodes, unfreeze_block\n        )\n        self.neck = Neck()\n        self.head = DetectionHead(\n            40, len(anchor_sizes) * len(anchor_ratios), num_classes\n        )\n\n        anchor_generator = AnchorGenerator(\n            sizes=(anchor_sizes, ), aspect_ratios=(anchor_ratios, ))\n        anchors = anchor_generator.grid_anchors([feature_map_size], strides=[[self.stride, self.stride]])\n        anchors = torch.stack(anchors, dim=0)\n        print(anchors.shape)\n        \n        anchors_xy = anchors[:, :, :2]\n        anchor_sizes = (anchors[:, :, 2:] - anchors[:, :, :2])\n\n        self.register_buffer(\"anchors\", anchors)\n        self.register_buffer(\"anchors_xy\", anchors_xy)\n        self.register_buffer(\"anchor_sizes\", anchor_sizes)\n\n    def forward(self, x):\n        x = self.backbone(x)\n\n        encoder_outputs = [x['0'], x['1'], x['2'], x['3'],\n                           x['4'], x['5'], x['6'], x['7'], x['8']]\n        x = self.neck(x['8'], encoder_outputs)\n        bbox_reg, confidence_score, cls_logits = self.head(x)\n\n        N = x.shape[0]\n        cls_logits = cls_logits.permute(0, 2, 3, 1).contiguous()\n        cls_logits = cls_logits.view(N, -1, self.head.num_classes)\n        bbox_reg = bbox_reg.permute(0, 2, 3, 1).contiguous()\n        bbox_reg = bbox_reg.view(N, -1, 4)\n        confidence_score = confidence_score.permute(0, 2, 3, 1).contiguous()\n        confidence_score = confidence_score.view(N, -1)\n\n        if self.training:\n            # В процессе тренировки возвращаем просто смещения и логиты\n            return bbox_reg, confidence_score, cls_logits\n\n        bbox = self.decode_bboxes(bbox_reg)\n        confidence_score = torch.sigmoid(confidence_score)\n        cls_probs = torch.softmax(cls_logits, dim=-1)\n\n        return bbox, confidence_score, cls_probs\n\n    def decode_bboxes(self, bbox_reg):\n        anchor_center_x = (self.anchors[:, :, 0] + self.anchors[:, :, 2]) / 2\n        anchor_center_y = (self.anchors[:, :, 1] + self.anchors[:, :, 3]) / 2\n        anchor_width = self.anchors[:, :, 2] - self.anchors[:, :, 0]\n        anchor_height = self.anchors[:, :, 3] - self.anchors[:, :, 1]\n        \n        # Предсказанные смещения якорей\n        tx = bbox_reg[:, :, 0]\n        ty = bbox_reg[:, :, 1]\n        tw = bbox_reg[:, :, 2]\n        th = bbox_reg[:, :, 3]\n        \n        x_center = anchor_center_x + torch.sigmoid(tx) * anchor_width\n        y_center = anchor_center_y + torch.sigmoid(ty) * anchor_height\n        w = torch.exp(tw) * anchor_width\n        h = torch.exp(th) * anchor_height\n        \n        # Переводим в [x_min, y_min, w, h] формат\n        x_min = x_center - w / 2\n        y_min = y_center - h / 2\n        return torch.stack([x_min, y_min, w, h], dim=-1)","metadata":{"id":"4dd8bcb8-149f-4916-b37f-8ba0be54a4c9","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:19:58.195326Z","iopub.execute_input":"2025-07-13T18:19:58.195681Z","iopub.status.idle":"2025-07-13T18:19:58.224607Z","shell.execute_reply.started":"2025-07-13T18:19:58.195647Z","shell.execute_reply":"2025-07-13T18:19:58.223679Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"## Label assignment","metadata":{"id":"9bc104b6-0960-404d-805c-ec880dc9bfb0"}},{"cell_type":"code","source":"def safe_logit(x):\n    \"\"\" Безопасная logit-функция(обратная сигмоиде) без деления на ноль. \"\"\"\n    eps = 1e-6\n    x = torch.clamp(x, eps, 1 - eps)\n    return torch.log(x / (1 - x))\n\n\ndef get_target_offset(anchor_box, gt_box):\n    \"\"\" Расчитываем таргет как желаемые смещения от якорей до GT.\n\n    anchor_box: torch.Tensor в формате (x_min, y_min, x_max, y_max),\n    gt_box: torch.Tensor в формате (x_min, y_min, x_max, y_max).\n    \"\"\"\n    # Конвертируем GT в формат (x_center, y_center), (w, h)\n    gt_center = (gt_box[:2] + gt_box[2:]) / 2\n    gt_size = gt_box[2:] - gt_box[:2]\n\n    # Конвертируем якоря в формат (x_center, y_center), (w, h)\n    anchor_center = (anchor_box[:2] + anchor_box[2:]) / 2\n    anchor_size = anchor_box[2:] - anchor_box[:2]\n\n    # Вычисляем значения смещений для положительных ббоксов\n    tx = (gt_center[0] - anchor_center[0]) / anchor_size[0]\n    ty = (gt_center[1] - anchor_center[1]) / anchor_size[1]\n    target_tx = safe_logit(tx)\n    target_ty = safe_logit(ty)\n\n    target_tw = torch.log(gt_size[0] / anchor_size[0])\n    target_th = torch.log(gt_size[1] / anchor_size[1])\n    return torch.tensor([target_tx, target_ty, target_tw, target_th]).to(anchor_box.device)","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-07-13T18:19:58.225378Z","iopub.execute_input":"2025-07-13T18:19:58.225706Z","iopub.status.idle":"2025-07-13T18:19:58.244290Z","shell.execute_reply.started":"2025-07-13T18:19:58.225678Z","shell.execute_reply":"2025-07-13T18:19:58.243126Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def assign_target(images, anchors, gt_boxes, gt_labels, num_classes, pos_th=0.6, neg_th=0.3, image_side_size=640):\n    num_anchors = anchors.shape[0]\n\n    target_objectness = torch.zeros(num_anchors, device=anchors.device)\n    target_offsets = torch.zeros((num_anchors, 4), device=anchors.device)\n    target_cls = torch.zeros((num_anchors, num_classes), device=anchors.device)\n    \n    # Если на изображении нет объектов, возвращаем пустые списки\n    if gt_boxes.numel() == 0:\n        return target_offsets, target_objectness, target_cls\n        \n    # box_iou работает с форматом ббоксов (x_min, y_min, x_max, y_max)\n    # Якоря находятся в нужном формате, а GT - нет, тк имеет формат (x_min, y_min, w, h)\n    # Переведем GT боксы в нужный формат\n    gt_xyxy = gt_boxes.clone()\n    gt_xyxy[:, 2:] = gt_xyxy[:, :2] + gt_xyxy[:, 2:]\n\n    # Считаем iou между всеми якорями и всеми GT\n    ious = box_iou(anchors, gt_xyxy)  # [num_anchors, num_gt]\n    # Находим самый оптимальный GT для каждого якоря\n    best_iou, best_gt_idx = ious.max(dim=1)\n\n    # Отмечаем якоря, которые будут пропущены при расчете лосса\n    ignore_mask = (best_iou >= neg_th) & (best_iou < pos_th)\n    ignore_indices = ignore_mask.nonzero(as_tuple=True)[0]\n    target_objectness[ignore_mask] = -1\n\n    # Отмечаем якоря, для которых будет считаться локализационный лосс\n    pos_mask = best_iou >= pos_th\n    pos_indices = pos_mask.nonzero(as_tuple=True)[0]\n    \n    # Отмечаем якоря, для которых будет считаться конфиденс лосс\n    neg_mask = best_iou < pos_th\n    neg_indices = neg_mask.nonzero(as_tuple=True)[0]\n\n    # print(\"Number of gt:\", len(gt_xyxy))\n    # print(\"Positive anchors:\", len(pos_indices))\n    # print(\"Negative anchors:\", len(neg_indices))\n    # print(\"Ignored anchors:\", len(ignore_indices))\n    # print(\"Max IoU:\", best_iou.max().item(), \"Mean IoU:\", best_iou.mean().item())\n\n    for pos in pos_indices:\n        gt_idx = best_gt_idx[pos]\n        gt_box = gt_xyxy[gt_idx]\n        anchor_box = anchors[pos]\n        # print(gt_box, anchor_box)\n        # plot_examples(images, targets=targets, image_norm=(mean, std), predictions=predicts, num_examples=min(batch_size, 3))\n\n        target_offsets[pos] = get_target_offset(anchor_box, gt_box)\n        target_objectness[pos] = 1\n        target_cls[pos, gt_labels[gt_idx]] = 1\n\n    # Присваиваем предсказание с самым большим IoU для GT\n    # у которых не нашлось ни оного предсказания\n    for gt_idx in range(gt_xyxy.shape[0]):\n        if not ((target_objectness == 1) & (best_gt_idx == gt_idx)).any():\n            best_anchor_idx = torch.argmax(ious[:, gt_idx])\n            target_offsets[best_anchor_idx] = get_target_offset(\n                anchors[best_anchor_idx], gt_xyxy[gt_idx])\n            target_objectness[best_anchor_idx] = 1\n            target_cls[best_anchor_idx, gt_labels[gt_idx]] = 1\n    return target_offsets, target_objectness, target_cls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:21:15.126262Z","iopub.execute_input":"2025-07-13T18:21:15.126595Z","iopub.status.idle":"2025-07-13T18:21:15.137673Z","shell.execute_reply.started":"2025-07-13T18:21:15.126571Z","shell.execute_reply":"2025-07-13T18:21:15.136811Z"}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"### Функция потерь","metadata":{"id":"28f25b75-58ed-44e7-adca-c5d518a7467b"}},{"cell_type":"code","source":"class ComputeLoss:\n\n    def __init__(self,\n                 bbox_loss=None, obj_loss=None, cls_loss=None,\n                 weight_bbox=5, weight_obj=1, weight_cls=1\n                 ):\n        self.bbox_loss = nn.SmoothL1Loss() if bbox_loss is None else bbox_loss\n        self.obj_loss = nn.BCEWithLogitsLoss() if obj_loss is None else obj_loss\n        self.cls_loss = nn.BCEWithLogitsLoss() if cls_loss is None else cls_loss\n        self.weight_bbox = weight_bbox\n        self.weight_obj = weight_obj\n        self.weight_cls = weight_cls\n\n    def __call__(self, predicts, targets):\n        pred_offsets, pred_obj_logits, pred_cls_logits = predicts\n        target_boxes, target_obj, target_cls = targets\n        # Confidence score считается только для предсказаний соотв отрицательным и положительным якорям\n        valid_mask = target_obj != -1\n        loss_obj = self.obj_loss(\n            pred_obj_logits[valid_mask], target_obj[valid_mask])\n\n        # Локализационная и классификационные части считаются для предсказаинй соотв положительным якорям\n        pos_mask = target_obj == 1\n        if pos_mask.sum() > 0:\n            loss_cls = self.cls_loss(\n                pred_cls_logits[pos_mask], target_cls[pos_mask])\n            loss_bbox = self.bbox_loss(\n                pred_offsets[pos_mask], target_boxes[pos_mask])\n        else:\n            loss_cls = torch.tensor(0.0, device=pred_offsets.device)\n            loss_bbox = torch.tensor(0.0, device=pred_offsets.device)\n        return self.weight_bbox * loss_bbox + self.weight_obj * loss_obj + self.weight_cls * loss_cls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:21:15.139180Z","iopub.execute_input":"2025-07-13T18:21:15.139424Z","iopub.status.idle":"2025-07-13T18:21:15.170337Z","shell.execute_reply.started":"2025-07-13T18:21:15.139406Z","shell.execute_reply":"2025-07-13T18:21:15.168926Z"}},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":"## Обучение детектора","metadata":{}},{"cell_type":"code","source":"class EarlyStopping:\n    \"\"\"\n    A utility class for early stopping during model training.\n    Monitors validation loss to halt training when the model stops improving.\n\n    Attributes:\n        patience (int): Number of epochs to wait for improvement before stopping.\n        delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n    \"\"\"\n\n    def __init__(self, patience: int = 5, delta: float = 0) -> None:\n        self.patience = patience\n        self.delta = delta\n        self.best_score = None\n        self.early_stop = False\n        self.counter = 0\n        self.best_model_state = None\n\n    def __call__(self, val_loss: int, model: nn.Module) -> None:\n        \"\"\"\n        Update the early stopping criteria based on the validation loss and the current model state.\n\n        Args:\n            val_loss (float): The current validation loss.\n            model (nn.Module): The model instance to store the state of.\n\n        Returns:\n            None\n        \"\"\"\n        score = -val_loss\n        if self.best_score is None:\n            self.best_score = score\n            self.best_model_state = model.state_dict()\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.best_model_state = model.state_dict()\n            self.counter = 0\n\n    def load_best_model(self, model: nn.Module) -> None:\n        \"\"\"\n        Load the best model state into the specified model.\n\n        Args:\n            model (nn.Module): The model instance to load the state into.\n\n        Returns:\n            None\n        \"\"\"\n        model.load_state_dict(self.best_model_state)","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-07-13T18:21:15.171877Z","iopub.execute_input":"2025-07-13T18:21:15.172235Z","iopub.status.idle":"2025-07-13T18:21:15.202725Z","shell.execute_reply.started":"2025-07-13T18:21:15.172203Z","shell.execute_reply":"2025-07-13T18:21:15.201730Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"class Runner:\n\n    def __init__(self, model, early_stopping, compute_loss, optimizer, train_dataloader, assign_target_method, device=None,\n                 scheduler=None, assign_target_kwargs=None,\n                 val_dataloader=None, val_every=1, score_threshold=0.1, nms_threshold=0.5, max_boxes_per_cls=8):\n        self.model = model\n        self.early_stopping = early_stopping\n        self.compute_loss = compute_loss\n        self.optimizer = optimizer\n        self.train_dataloader = train_dataloader\n        assign_target_kwargs = {} if assign_target_kwargs is None else assign_target_kwargs\n        self.assign_target_method = partial(\n            assign_target_method, **assign_target_kwargs)\n        self.device = \"cpu\" if device is None else device\n        self.scheduler = scheduler\n\n        # Валидационные параметры\n        self.val_dataloader = val_dataloader\n        self.val_every = val_every\n        self.score_threshold = score_threshold\n        self.nms_threshold = nms_threshold\n        self.max_boxes_per_cls = max_boxes_per_cls\n\n        # Вспомогательные массивы\n        self.train_batch_loss = []\n        self.val_batch_loss = []\n        self.epoch_loss = []\n        self.val_metric = []\n\n    def _run_train_epoch(self, dataloader, verbose=True):\n        \"\"\" Обучить модель одну эпоху на данных из `dataloader` \"\"\"\n        self.model.train()\n        batch_loss = []\n        for images, targets in (pbar := tqdm(dataloader, desc=f\"Process train epoch\", leave=False)):\n            images = images.to(self.device)\n            outputs = self.model(images)\n\n            anchors = self.model.anchors.view(-1, 4)\n            accum_loss = 0.0\n            for ix in range(images.shape[0]):\n                gt_boxes = targets[ix]['boxes'].to(self.device)\n                gt_labels = targets[ix]['labels'].to(self.device)\n                # выбираем какие якоря будут использоваться при расчете лосса.\n                assigned_targets = self.assign_target_method(images, anchors, gt_boxes, gt_labels,\n                                                             num_classes=self.model.head.num_classes)\n                # Считаем лосс на основании предсказаний модели и таргетов.\n                outputs_ixs = [out[ix] for out in outputs]\n                loss = self.compute_loss(outputs_ixs, assigned_targets)\n                accum_loss += loss\n            accum_loss = accum_loss / images.shape[0]\n            batch_loss.append(accum_loss.cpu().detach().item())\n\n            # Делаем шаг оптимизатора после расчета лосса для всех элементов батча\n            self.optimizer.zero_grad()\n            accum_loss.backward()\n            self.optimizer.step()\n        # Обновляем описание tqdm бара усредненным значением лосса за предыдущй батч\n            if verbose:\n                pbar.set_description(f\"Current batch loss: {batch_loss[-1]:.4}\")\n        return batch_loss\n\n    def train(self, num_epochs=10, verbose=True):\n        \"\"\" Обучаем модель заданное количество эпох. \"\"\"\n        val_desc = \"\"\n\n        for epoch in (epoch_pbar := tqdm(range(1, num_epochs+1), desc=\"Train epoch\", total=num_epochs)):\n            # Обучаем модель одну эпоху\n            loss = self._run_train_epoch(\n                self.train_dataloader, verbose=verbose)\n            self.train_batch_loss.extend(loss)\n            self.epoch_loss.append(\n                np.mean(self.train_batch_loss[-len(self.train_dataloader):]))\n\n            # Делаем валидацию, если был передан валидационный датасет\n            if self.val_dataloader is not None and epoch % self.val_every == 0:\n                val_metric, avg_val_loss = self.validate()\n                self.val_metric.append(val_metric)\n                self.val_batch_loss.append(avg_val_loss)\n                val_desc = f\" valid mAP {val_metric}, val loss {avg_val_loss}\"\n\n                self.early_stopping(avg_val_loss, self.model)\n                if self.early_stopping.early_stop:\n                    print(\"Early stopping\")\n                    break\n            else:\n                val_desc = ''\n\n            # Обновляем описание tqdm бара усредненным значением лосса за предыдую эпоху\n            if verbose:\n                epoch_pbar.set_description(\n                    f\"{epoch} epoch train loss: {self.epoch_loss[-1]:.4}\" + val_desc)\n            # Делаем шаг scheduler'a если он был передан\n            if self.scheduler is not None:\n                self.scheduler.step()\n\n        early_stopping.load_best_model(model)\n\n    @torch.no_grad()\n    def validate(self, dataloader=None):\n        \"\"\" Метод для валидации модели. Если dataloader не передан, будет использоваться self.val_dataloder.\n        Возвращает mAP (0.5 ... 0.95).\n        \"\"\"\n        self.model.eval()\n        dataloader = self.val_dataloader if dataloader is None else dataloader\n        total_loss = 0.0\n        num_batches = 0\n        \n        # Считаем метрику mAP с помощью функции из torchmetrics\n        metric = MeanAveragePrecision(box_format=\"xywh\", iou_type=\"bbox\")\n\n        max_score = 0\n        for images, targets in tqdm(dataloader, desc=\"Running validation\", leave=False):\n            images = images.to(self.device)\n            outputs = self.model(images)\n\n            predicts = _filter_predictions(outputs, self.score_threshold, self.nms_threshold,\n                                           max_boxes_per_cls=self.max_boxes_per_cls, return_type=\"torch\")\n\n            # DEBUG: max score\n            bboxes, confidences, cls_probs = outputs\n            num_classes = cls_probs.shape[-1]\n            all_final_scores = confidences[:, :, None] * cls_probs\n            max_score = max(all_final_scores.max().item(), max_score)\n            # plot_examples(images, targets=targets, image_norm=(mean, std), predictions=predicts, num_examples=min(batch_size, 3))\n\n            metric.update(predicts, targets)\n\n            # Compute validation loss\n            anchors = self.model.anchors.view(-1, 4)\n            batch_loss = 0.0\n            for ix in range(images.shape[0]):\n                gt_boxes = targets[ix]['boxes'].to(self.device)\n                gt_labels = targets[ix]['labels'].to(self.device)\n                assigned_targets = self.assign_target_method(\n                    images, anchors, gt_boxes, gt_labels, num_classes=self.model.head.num_classes\n                )\n                outputs_ixs = [out[ix] for out in outputs]\n                loss = self.compute_loss(outputs_ixs, assigned_targets)\n                batch_loss += loss.item()\n            total_loss += batch_loss / images.shape[0]\n            num_batches += 1\n            \n        print(\"max_score:\", max_score)\n            \n        avg_val_loss = total_loss / num_batches\n        map_value = metric.compute()[\"map\"].item()\n        \n        return map_value, avg_val_loss\n\n    def plot_loss(self, row_figsize=3):\n        nrows = 3 if self.val_metric else 1\n        _, ax = plt.subplots(nrows, 1, figsize=(\n            12, row_figsize*nrows), tight_layout=True)\n        ax = np.array([ax]) if not isinstance(ax, np.ndarray) else ax\n        ax[0].plot(self.train_batch_loss, label=\"Train batch Loss\", color=\"tab:blue\")\n        ax[0].plot(np.linspace(1, len(self.train_batch_loss), len(self.epoch_loss)), self.epoch_loss,\n                   color=\"tab:orange\", label=\"Train epoch Loss\")\n        ax[0].grid()\n        ax[0].set_title(\"Train Loss\")\n        ax[0].set_xlabel(\"Number of Iterations\")\n        ax[0].set_ylabel(\"Loss\")\n        if self.val_metric:\n            ax[1].plot(np.linspace(self.val_every, len(self.train_batch_loss), len(self.val_metric)),\n                       np.array(self.val_metric) * 100, color=\"tab:green\", label=\"Validation mAP\")\n            ax[1].grid()\n            ax[1].set_title(\"Valiation mAP\")\n            ax[1].set_xlabel(\"Number of Iterations\")\n            ax[1].set_ylabel(\"mAP (%)\")\n\n            ax[2].plot(np.linspace(self.val_every, len(self.train_batch_loss), len(self.val_metric)),\n                       np.array(self.val_batch_loss), color=\"tab:red\", label=\"Validation Loss\")\n            ax[2].grid()\n            ax[2].set_title(\"Validation Loss\")\n            ax[2].set_xlabel(\"Number of Iterations\")\n            ax[2].set_ylabel(\"Loss\")\n        plt.legend()\n        plt.show()\n\n\ndef _filter_predictions(predictions, score_threshold=0.1, nms_threshold=0.5, max_boxes_per_cls=8, return_type=\"list\"):\n    \"\"\" Ббоксы в `predictions` должны быть в формате (x_min, y_min, w, h). \"\"\"\n    # Итоговый скор считается как произведение уверенности модели в том что в данном якоре\n    # и вероятность каждого класса в данном якоре.\n    bboxes, confidences, cls_probs = predictions\n    all_final_scores = confidences[:, :, None] * cls_probs\n\n    num_classes = cls_probs.shape[-1]\n    final_predictions = []\n    \n    # Для каждого элемента в `predictions` независимо выбираем ббоксы и скоры\n    for boxes, final_scores in zip(bboxes, all_final_scores):\n        preds = {\"boxes\": [], \"labels\": [], \"scores\": []}\n\n        # Для каждого класса отдельно фильтруем ббоксы с помощью NMS\n        for cls in range(num_classes):\n            cls_scores = final_scores[:, cls]\n            \n            # Фильтруем ббоксы, score которых меньше порога\n            keep_ixs = cls_scores > score_threshold\n            if keep_ixs.sum() == 0:\n                continue\n            cls_boxes = boxes[keep_ixs]\n            cls_scores = cls_scores[keep_ixs]\n\n            # Если предсказаний слишком много, выбираем только самые уверенные\n            if len(cls_boxes) > max_boxes_per_cls:\n                pos = torch.argsort(cls_scores, descending=True)\n                cls_boxes = cls_boxes[pos[:max_boxes_per_cls]]\n                cls_scores = cls_scores[pos[:max_boxes_per_cls]]\n\n            # Конвертируем ббоксы в формат x_min, y_min, x_max, y_max из COCO\n            boxes_xyxy = cls_boxes.clone()\n            boxes_xy_min = boxes_xyxy[:, :2] - boxes_xyxy[:, 2:]\n            boxes_xy_max = boxes_xyxy[:, :2] + boxes_xyxy[:, 2:]\n            boxes_xyxy[:, :2] = boxes_xy_min\n            boxes_xyxy[:, 2:] = boxes_xy_max\n            # Запускаем NMS по всем оставшимся ббоксам класса cls\n            pred_ixs = nms(boxes_xyxy, cls_scores, nms_threshold)\n            # Сохраняем все предсказания для класса cls\n            for ix in pred_ixs:\n                preds[\"boxes\"].append(cls_boxes[ix].cpu().tolist())\n                preds[\"labels\"].append(cls)\n                preds[\"scores\"].append(cls_scores[ix].item())\n        if return_type == \"torch\":\n            for key, item in preds.items():\n                preds[key] = torch.tensor(item)\n        elif return_type != \"list\":\n            raise ValueError(\n                f\"Received unexpected `return_type`. Could be either `torch` or `list`, not {return_type}\")\n\n        final_predictions.append(preds)\n    return final_predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:26:05.317505Z","iopub.execute_input":"2025-07-13T18:26:05.318407Z","iopub.status.idle":"2025-07-13T18:26:05.350316Z","shell.execute_reply.started":"2025-07-13T18:26:05.318378Z","shell.execute_reply":"2025-07-13T18:26:05.349347Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# Расчитываем среднюю высоту и ширину для таргетов\nall_widths = []\nall_heights = []\nfor images, targets in tqdm(train_dataset):\n    for i in range(images.shape[0]):\n        boxes = targets['boxes']\n        all_widths.extend(boxes[:, 2].tolist())\n        all_heights.extend(boxes[:, 3].tolist())\n\nmean_w = np.mean(all_widths)\nmean_h = np.mean(all_heights)\nprint(f\"Mean GT box size: {mean_w:.1f}x{mean_h:.1f} pixels\")\n\nfrom sklearn.cluster import KMeans\n\n# Делим размеры на n кластеров\ngt_sizes = np.array([[w, h] for w, h in zip(all_widths, all_heights)])\nkmeans = KMeans(n_clusters=3, random_state=0).fit(gt_sizes)\ncluster_centers = kmeans.cluster_centers_  # Shape: (5, 2)\n\n# Получаем средний размер каждого кластера\nbase_sizes = np.mean(cluster_centers, axis=1).tolist()\nbase_sizes = tuple((int(x), ) for x in base_sizes)\n\naspect_ratios = ((0.5, 1, 2), ) * len(base_sizes)\n\nprint(\"base_sizes\", base_sizes)\nprint(\"aspect_ratios\", aspect_ratios)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:21:15.284758Z","iopub.execute_input":"2025-07-13T18:21:15.285078Z","iopub.status.idle":"2025-07-13T18:21:16.184849Z","shell.execute_reply.started":"2025-07-13T18:21:15.285057Z","shell.execute_reply":"2025-07-13T18:21:16.184173Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/42 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d64d5efc0d564988a1352b69bd67f662"}},"metadata":{}},{"name":"stdout","text":"Mean GT box size: 36.0x23.6 pixels\nbase_sizes ((22,), (42,), (76,))\naspect_ratios ((0.5, 1, 2), (0.5, 1, 2), (0.5, 1, 2))\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"lr = 1e-3\n\nmodel = Detector(\n    pretrained_model,\n    return_nodes,\n    # anchor_sizes=base_sizes,\n    # anchor_ratios=aspect_ratios,\n    unfreeze_block=2,\n    num_classes=len(four_classes),\n    input_size=(img_width, img_height)\n).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer, T_max=10, eta_min=5e-5)\n\nsmooth_l1_loss = nn.SmoothL1Loss()\nobj_bce_loss = nn.BCEWithLogitsLoss()\ncls_bce_loss = nn.BCEWithLogitsLoss()\ncompute_loss = ComputeLoss(\n    bbox_loss=smooth_l1_loss,\n    obj_loss=obj_bce_loss,\n    cls_loss=cls_bce_loss,\n    weight_bbox=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:21:16.185422Z","iopub.execute_input":"2025-07-13T18:21:16.185670Z","iopub.status.idle":"2025-07-13T18:21:16.557463Z","shell.execute_reply.started":"2025-07-13T18:21:16.185650Z","shell.execute_reply":"2025-07-13T18:21:16.556440Z"}},"outputs":[{"name":"stdout","text":"feature_map_size (40, 40)\ntorch.Size([1, 14400, 4])\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"early_stopping = EarlyStopping(patience=15, delta=0.0001)\nrunner = Runner(\n    model,\n    early_stopping,\n    compute_loss,\n    optimizer,\n    train_dataloader,\n    assign_target,\n    device=device,\n    scheduler=scheduler,\n    assign_target_kwargs={\"neg_th\": 0.4, \"pos_th\": 0.5},\n    val_dataloader=train_dataloader,\n    score_threshold=0.1\n)\n\nnum_epochs = 1","metadata":{"id":"4f0b3ca5-c7cb-4bfd-a999-a88df9ee56b8","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:22:19.118680Z","iopub.execute_input":"2025-07-13T18:22:19.119032Z","iopub.status.idle":"2025-07-13T18:22:19.125168Z","shell.execute_reply.started":"2025-07-13T18:22:19.119009Z","shell.execute_reply":"2025-07-13T18:22:19.124151Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()\nprint(torch.cuda.memory_allocated())  # Память, выделенная для тензоров\nprint(torch.cuda.memory_reserved()) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:22:19.126779Z","iopub.execute_input":"2025-07-13T18:22:19.127115Z","iopub.status.idle":"2025-07-13T18:22:19.469533Z","shell.execute_reply.started":"2025-07-13T18:22:19.127093Z","shell.execute_reply":"2025-07-13T18:22:19.468765Z"}},"outputs":[{"name":"stdout","text":"0\n0\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"runner.train(num_epochs=num_epochs, verbose=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:22:19.470973Z","iopub.execute_input":"2025-07-13T18:22:19.471818Z","iopub.status.idle":"2025-07-13T18:25:07.699354Z","shell.execute_reply.started":"2025-07-13T18:22:19.471788Z","shell.execute_reply":"2025-07-13T18:25:07.697683Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Train epoch:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6607a769bdcf431b91cebc69852e7594"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Process train epoch:   0%|          | 0/21 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running validation:   0%|          | 0/21 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f7e3b479552456885367853eb245df6"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/4012162470.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_36/3643542127.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs, verbose)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;31m# Делаем валидацию, если был передан валидационный датасет\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dataloader\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mval_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_metric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_batch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_val_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/3643542127.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self, dataloader)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mgt_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'boxes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0mgt_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 assigned_targets = self.assign_target_method(\n\u001b[0m\u001b[1;32m    135\u001b[0m                     \u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 )\n","\u001b[0;31mTypeError\u001b[0m: assign_target() missing 1 required positional argument: 'gt_labels'"],"ename":"TypeError","evalue":"assign_target() missing 1 required positional argument: 'gt_labels'","output_type":"error"}],"execution_count":49},{"cell_type":"code","source":"torch.save(model.state_dict(), \"model.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:25:07.699886Z","iopub.status.idle":"2025-07-13T18:25:07.700279Z","shell.execute_reply.started":"2025-07-13T18:25:07.700140Z","shell.execute_reply":"2025-07-13T18:25:07.700154Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"runner.plot_loss(row_figsize=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:25:07.701197Z","iopub.status.idle":"2025-07-13T18:25:07.701469Z","shell.execute_reply.started":"2025-07-13T18:25:07.701344Z","shell.execute_reply":"2025-07-13T18:25:07.701356Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Расчёт метрик","metadata":{}},{"cell_type":"code","source":"from torchmetrics.detection import MeanAveragePrecision\n\n@torch.no_grad()\ndef validate(dataloader, filter_predictions_func, box_format=\"xyxy\", device=\"cpu\", score_threshold=0.1, nms_threshold=0.5, **kwargs):\n    \"\"\" Метод для валидации модели.\n    Возвращает mAP (0.5 ... 0.95).\n    \"\"\"\n    self.model.eval()\n    # Считаем метрику mAP с помощью функции из torchmetrics\n    metric = MeanAveragePrecision(box_format=box_format, iou_type=\"bbox\")\n    for images, targets in tqdm(dataloader, desc=\"Running validation\", leave=False):\n        images = images.to(device)\n        outputs = self.model(images)\n        predicts = filter_predictions_func(outputs, score_threshold, nms_threshold, **kwargs)\n        metric.update(predicts, targets)\n    return metric.compute()[\"map\"].item()\n","metadata":{"id":"299b193f-b697-449d-b4c0-3ef57c77eb7e","trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-07-13T18:25:07.702913Z","iopub.status.idle":"2025-07-13T18:25:07.703342Z","shell.execute_reply.started":"2025-07-13T18:25:07.703132Z","shell.execute_reply":"2025-07-13T18:25:07.703150Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef predict(model, images, device, score_threshold=0.1, nms_threshold=0.5, max_boxes_per_cls=8, return_type='list'):\n    \"\"\" Предсказание моделью для переданного набора изображений после фильтрации по score_threshold\n    и применения NMS.\n\n    Параметры\n    --------\n    images : torch.tensor, содержащий картинки для которых нужно сделать предсказание.\n    Необходимые преобразования должны быть сделаны ДО. Внутри метода `predict` никаких преобразований\n    не происходит.\n    score_threshold : Все предсказания, с (confidence score * cls_probs) < score_threshold будут проигнорированны.\n    nms_threshold : Предсказания, имеющие пересечение по IoU >= nms_threshold будут считаться одним предсказанием.\n    max_boxes_per_cls : Максимальное количество ббоксов на изображение для одного класса после фильтрации по `score_threshold`.\n\n    Returns\n    -------\n    final_predictions : List[dict], где каждый словарь содержащий следующие ключи:\n        \"boxes\" : координаты ббоксов на i-ом изображении,\n        \"labels\" : классы внутри ббоксов,\n        \"scores\" : Confidence scores для ббоксов.\n    \"\"\"\n    model.eval()\n    images = images.to(device)\n    outputs = model(images)\n    final_predictions =  _filter_predictions(outputs, score_threshold=score_threshold, nms_threshold=nms_threshold,\n                                             max_boxes_per_cls=max_boxes_per_cls, return_type=return_type)\n    return final_predictions","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-07-13T18:25:07.705554Z","iopub.status.idle":"2025-07-13T18:25:07.706057Z","shell.execute_reply.started":"2025-07-13T18:25:07.705813Z","shell.execute_reply":"2025-07-13T18:25:07.705863Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_iter = iter(train_dataloader)  # DEBUG: test_dataloader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:25:07.707966Z","iopub.status.idle":"2025-07-13T18:25:07.708354Z","shell.execute_reply.started":"2025-07-13T18:25:07.708199Z","shell.execute_reply":"2025-07-13T18:25:07.708213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"score_threshold = 0.15\nnms_threshold = 0.1\n\nimages, targets = next(test_iter)\npreds = predict(model, images, device=device, score_threshold=score_threshold, nms_threshold=nms_threshold)\n\nplot_examples(images, targets=targets, image_norm=(mean, std), predictions=preds, num_examples=min(len(images), 3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:25:07.709884Z","iopub.status.idle":"2025-07-13T18:25:07.710280Z","shell.execute_reply.started":"2025-07-13T18:25:07.710058Z","shell.execute_reply":"2025-07-13T18:25:07.710070Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Final mAP:\", runner.validate())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:25:07.712442Z","iopub.status.idle":"2025-07-13T18:25:07.712790Z","shell.execute_reply.started":"2025-07-13T18:25:07.712652Z","shell.execute_reply":"2025-07-13T18:25:07.712668Z"}},"outputs":[],"execution_count":null}]}